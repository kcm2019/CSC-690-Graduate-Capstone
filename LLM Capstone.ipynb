{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone LLM Project\n",
    "This is going to be a beginner LLM from scratch using Python  \n",
    "Need to install the following packages:  \n",
    "- matplotlib\n",
    "- numpy\n",
    "- pylzma\n",
    "- ipykernel\n",
    "- jupyter\n",
    "- torch\n",
    "\n",
    "## MAKE SURE TO SELECT ANACANDA 3.11 ENVIRONMENT TO RUN\n",
    "\n",
    "VIDEO LINK: https://www.youtube.com/watch?v=UU1WVnMk4E8\n",
    "LEFT OFF ON 2:31:08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "#!pip install matplotlib numpy pylzma ipykernel jupyter torchvision torchaudio torch numpy\n",
    "\n",
    "# Import packages\n",
    "import matplotlib\n",
    "import numpy\n",
    "import ipykernel\n",
    "import jupyter\n",
    "import torch\n",
    "import torch\n",
    "import pylzma\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4456, 0.6170, 0.3287],\n",
      "        [0.9728, 0.9239, 0.8149],\n",
      "        [0.8035, 0.2152, 0.1445],\n",
      "        [0.2647, 0.7729, 0.6845],\n",
      "        [0.1472, 0.7767, 0.1293]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "with open(\"wizardofoz.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([79, 26, 31, 24, 39, 43, 28, 41,  1, 12, 10,  0,  0, 43, 31, 28,  1, 28,\n",
      "        24, 41, 43, 31, 40, 44, 24, 34, 28,  0,  0,  0, 43, 60, 57,  1, 72, 70,\n",
      "        53, 61, 66,  1, 58, 70, 67, 65,  1,  5, 29, 70, 61, 71, 55, 67,  1, 75,\n",
      "        53, 71,  1, 74, 57, 70, 77,  1, 64, 53, 72, 57, 10,  1, 32, 72,  1, 71,\n",
      "        60, 67, 73, 64, 56,  1, 60, 53, 74, 57,  1, 53, 70, 70, 61, 74, 57, 56,\n",
      "         1, 53, 72,  1, 31, 73, 59, 71, 67, 66])\n"
     ]
    }
   ],
   "source": [
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "\"\"\" encoded_hello = encode('hello')\n",
    "decoded_hello = decode([60, 57, 64, 64, 67])\n",
    "\n",
    "print(encoded_hello)\n",
    "print(decoded_hello) \"\"\"\n",
    "\n",
    "#Tensor likes having everything in tensor objects\n",
    "data = torch.tensor(encode(text), dtype=torch.long) #This is just going to be a super long sequence of integers\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation and Training Sets (Bigram Language Model)\n",
    "We want to take a given text, split it up and train the model on 80% of it, and then use the remaining 20% to validate the training. If it was trained on the entire text, it would eventuall memeroize the entire training set and just spit that back out. The purpose of the model is to output text that is like the training data. This is why they are put into splits. Doing 80/20, it makes sure the splits are unique. We generate something like the trained text, but not it exactly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation and Training Splits\n",
    "n = int(0.8*len(data)) #80% of the text is training, 20% is validating\n",
    "train_data = data[:n]\n",
    "val_data = data [n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([79]) target is tensor(26)\n",
      "when input is tensor([79, 26]) target is tensor(31)\n",
      "when input is tensor([79, 26, 31]) target is tensor(24)\n",
      "when input is tensor([79, 26, 31, 24]) target is tensor(39)\n",
      "when input is tensor([79, 26, 31, 24, 39]) target is tensor(43)\n",
      "when input is tensor([79, 26, 31, 24, 39, 43]) target is tensor(28)\n",
      "when input is tensor([79, 26, 31, 24, 39, 43, 28]) target is tensor(41)\n",
      "when input is tensor([79, 26, 31, 24, 39, 43, 28, 41]) target is tensor(1)\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range (block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print('when input is', context, 'target is', target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing CPU vs GPU Runtime For Multiplying Matrixes\n",
    "\n",
    "With simple tasks, it is not very hard to do, like adding a lot of things up. The issue with running it on a CPU, is that it will run sequentially. With the GPU, it can perform all these tasks in parallel. This is why we need to run the above code in parallel. Below is an example showing the speed of CPU and GPU computation for a lot of simple tasks and the time difference between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU:  0.7807\n",
      "CPU:  1.1141\n"
     ]
    }
   ],
   "source": [
    "#Setting up GPU use\n",
    "import os\n",
    "\n",
    "def check_operating_system():\n",
    "    if os.name == 'posix':\n",
    "        # On Unix-like systems (including macOS)\n",
    "        return 'macOS' if 'Darwin' in os.uname() else 'Linux'\n",
    "    elif os.name == 'nt':\n",
    "        # On Windows\n",
    "        return 'Windows'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Example usage\n",
    "operating_system = check_operating_system()\n",
    "if operating_system == 'macOS': device = 'mps' #This is for M1 Macs or newer\n",
    "elif operating_system == 'Windows' : device = 'cuda' if torch.cuda_is_available() else 'cpu' #Must set up cuda with torch to run if on windows, but this project was developed and made for Mac\n",
    "\n",
    "#print(device)\n",
    "\n",
    "mps_device = torch.device(device)\n",
    "\n",
    "batch_size = 8\n",
    "block_size = 4\n",
    "\n",
    "#Comparing Numpy (CPU) against Torch (GPU)\n",
    "torch_rand1 = torch.rand(100, 100, 100, 100).to(mps_device)\n",
    "torch_rand2 = torch.rand(100, 100, 100, 100).to(mps_device)\n",
    "np_rand1 = torch.rand(100, 100, 100, 100)\n",
    "np_rand2 = torch.rand(100, 100, 100, 100)\n",
    "\n",
    "#With GPU - Much Faster\n",
    "start_time = time.time()\n",
    "rand = (torch_rand1 @ torch_rand2)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"GPU: \", f\"{elapsed_time:.4f}\")\n",
    "\n",
    "#With CPU - Much slower\n",
    "start_time = time.time()\n",
    "rand = np.multiply(np_rand1, np_rand2)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"CPU: \", f\"{elapsed_time:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Check that MPS is available\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    mps_device = torch.device(\"mps\")\n",
    "\n",
    "    # Create a Tensor directly on the mps device\n",
    "    x = torch.ones(5, device=mps_device)\n",
    "    # Or\n",
    "    x = torch.ones(5, device=\"mps\")\n",
    "\n",
    "    # Any operation happens on the GPU\n",
    "    y = x * 2\n",
    "    print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to our Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_iters = 10000\n",
    "learning_rate = 3e-4 # You have to experiment with learning rate\n",
    "eval_iters = 250\n",
    "#dropout = 0.2 #drops out random nuerons in the network to help the model learn better when things are not in the right place, takes 20% nuerons out at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "tensor([[53, 66, 55, 57, 56,  1, 61, 66],\n",
      "        [ 1, 77, 67, 73,  1, 65, 73, 71],\n",
      "        [ 1, 53, 71,  1, 75, 57, 64, 64],\n",
      "        [77,  1, 64, 57, 53, 66, 57, 56]], device='mps:0')\n",
      "targets: \n",
      "tensor([[66, 55, 57, 56,  1, 61, 66,  0],\n",
      "        [77, 67, 73,  1, 65, 73, 71, 72],\n",
      "        [53, 71,  1, 75, 57, 64, 64,  1],\n",
      "        [ 1, 64, 57, 53, 66, 57, 56,  1]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    #print(ix)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(mps_device), y.to(mps_device) #Processes on the GPU, in paralell\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs: ')\n",
    "#print(x.shape)\n",
    "print(x)\n",
    "print('targets: ')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I!Sf﻿ZHGEKYL1PdA75vGR﻿3UAv&W0oVq2X4(Mhr!\"2JTt-&[un&OVOq&8-Y3b\n",
      "?f_ALa_9VBeM]uHE_l y(10W\"i?fUk-TpX4Dq7yd!wsl\n",
      "2X.h0EG436848f7YU6_pZWuUishKtSR7,E1bjmQg0HxuT\"HE)c0Y&\n",
      "h:-Z;6t\"tA\"\n",
      "5O_EUNRdk﻿V'srCCRfyxz,&ffygQw] [uD.rA8pYLwbwecPYKwv?,n&xMQ[B3UA8-r;kysY0UM_m;q\"cx)WcDEzh\n",
      "x5OQ6!:\"cXA!mz0h,DT3uO0Jvje7ZA2va?f7JtS [﻿iX.SRm(yHEiz4X&n5'l﻿Z2\"Q\"wb_oV)5k&A.2oQpsLyxkMXPu10D&Q7jQ:(UMkAL'L,:53x8-:q_)sY veBb-TiP-pbj\"Xqd.NkUo?flADIZf!:9QRRa daK4WGopPw?Usw ,wZT4PNo&oqPW1e4vhjn)﻿5!:IdQYTG-mgY;WsnKt&_lD\"z3?T:Y_Vq?J6e:(DIz\n"
     ]
    }
   ],
   "source": [
    "#AdamW is an optimizer of Torch, using gradient descent to optimize parameters and make sure certain parameters aren't affecting performance too much, using nn.Module allows us to use that\n",
    "class BigramLanguageModel(nn.Module): #nn.Module allows us to make parameters learnable\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # Embedding table, giant grid of the probability distribution of what character comes next based on the previous character\n",
    "        \n",
    "    def forward(self, index, targets=None):\n",
    "        # Logits are a bunch of normalized floating numbers\n",
    "        logits = self.token_embedding_table(index) \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            #Batch, Time, Channels for cross entropy b/c it expects a certain shape\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        #index is (B, T) array of indicies in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            #Get predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            \n",
    "            #Focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            \n",
    "            #Apply Softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)# (B, C)\n",
    "            \n",
    "            #Sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            #Append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(mps_device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=mps_device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a PyTorch Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss 4.8371, val loss: 4.8480\n",
      "step: 250, train loss 4.7648, val loss: 4.7869\n",
      "step: 500, train loss 4.7019, val loss: 4.7142\n",
      "step: 750, train loss 4.6456, val loss: 4.6472\n",
      "step: 1000, train loss 4.5821, val loss: 4.5913\n",
      "step: 1250, train loss 4.5328, val loss: 4.5415\n",
      "step: 1500, train loss 4.4441, val loss: 4.4800\n",
      "step: 1750, train loss 4.4193, val loss: 4.4407\n",
      "step: 2000, train loss 4.3512, val loss: 4.3900\n",
      "step: 2250, train loss 4.2984, val loss: 4.3371\n",
      "step: 2500, train loss 4.2362, val loss: 4.2782\n",
      "step: 2750, train loss 4.2063, val loss: 4.2037\n",
      "step: 3000, train loss 4.1546, val loss: 4.1547\n",
      "step: 3250, train loss 4.0839, val loss: 4.1261\n",
      "step: 3500, train loss 4.0488, val loss: 4.0919\n",
      "step: 3750, train loss 4.0206, val loss: 4.0494\n",
      "step: 4000, train loss 3.9745, val loss: 3.9872\n",
      "step: 4250, train loss 3.9167, val loss: 3.9697\n",
      "step: 4500, train loss 3.8794, val loss: 3.8989\n",
      "step: 4750, train loss 3.8307, val loss: 3.8852\n",
      "step: 5000, train loss 3.8192, val loss: 3.8366\n",
      "step: 5250, train loss 3.7498, val loss: 3.7637\n",
      "step: 5500, train loss 3.6946, val loss: 3.7453\n",
      "step: 5750, train loss 3.6871, val loss: 3.6892\n",
      "step: 6000, train loss 3.6299, val loss: 3.6603\n",
      "step: 6250, train loss 3.5929, val loss: 3.6151\n",
      "step: 6500, train loss 3.5591, val loss: 3.5677\n",
      "step: 6750, train loss 3.5315, val loss: 3.5689\n",
      "step: 7000, train loss 3.4688, val loss: 3.5152\n",
      "step: 7250, train loss 3.4749, val loss: 3.4837\n",
      "step: 7500, train loss 3.4157, val loss: 3.4661\n",
      "step: 7750, train loss 3.3765, val loss: 3.3960\n",
      "step: 8000, train loss 3.3530, val loss: 3.4009\n",
      "step: 8250, train loss 3.3429, val loss: 3.3752\n",
      "step: 8500, train loss 3.2872, val loss: 3.3412\n",
      "step: 8750, train loss 3.2770, val loss: 3.2963\n",
      "step: 9000, train loss 3.2534, val loss: 3.3063\n",
      "step: 9250, train loss 3.2219, val loss: 3.2735\n",
      "step: 9500, train loss 3.2040, val loss: 3.2291\n",
      "step: 9750, train loss 3.1828, val loss: 3.2371\n",
      "3.1290054321289062\n"
     ]
    }
   ],
   "source": [
    "#Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    #sample from a batch of data\n",
    "    if iter % eval_iters == 0:\n",
    "        losses=estimate_loss()\n",
    "        print(f\"step: {iter}, train loss {losses['train']:.4f}, val loss: {losses['val']:.4f}\")\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    #evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      ")Vd t-s\"MK?ffaf;B-nt DK]0SF﻿unosyG2ft\n",
      "O3npA!Ybo\n",
      "\n",
      "?JH4URthm3CCrB)Ce!8F﻿9VlN18&e s!qCl\n",
      "ar\n",
      "TH﻿3fig]d t!:(1GOqSsav_mib]0Loz\"NWK!Whe\n",
      "\"Hd.DV?fategOlfO8GTLamJfO[Yua qFck\n",
      "\"oRH\"﻿y_(lAnng-K8atI, s\n",
      "\n",
      "In w eownoWb]2lind igjn.DQ?pon zinryALWmatKSatq?f\n",
      "he,&D[rom,t5JS d_﻿91ROsl\n",
      "TO'J!jxKm4'8GFn snt xRrusaipU:R3' ffl,)b\n",
      "4un3)sa XAJdualfIdls!:t]0le g?fC79un d9Z23f58-.;EDTL1 zH\n",
      "\"﻿7J2lyllWKT60b\n",
      "OY;bve.L!y)9P1_s,!-Yx8a,NzacHG_lk&?f\n",
      "TVE:wruf\n",
      "y XvfXGO\n",
      "?z is﻿ F8l_8upouxOQ).ntau, idf7 7RHirrl58BH,xs?fiY1_V4E)YHS!-pz-HFh\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=mps_device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How is our model performing over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "#Define a probability tensor\n",
    "probabilities = torch.tensor([0.1, 0.9]) #each side must add up to 1 for 100%\n",
    "#10% or 0.1 => 0, 90% or 0.9 => 1. each probability points to the index of the probability in the tensor\n",
    "#Draw 5 samples from the multinomial distribution\n",
    "samples = torch.multinomial(probabilities, num_samples=10, replacement=True)\n",
    "print(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "#import torch\n",
    "\n",
    "tensor = torch.tensor([1, 2, 3, 4])\n",
    "out = torch.cat((tensor, torch.tensor([5])), dim=0)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "tensor([[0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "torch.Size([4, 3, 2])\n",
      "Stacked Tensor: tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor([-9.1271,  4.9377,  2.9602], grad_fn=<SqueezeBackward4>)\n",
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "#Showing more features of torch\n",
    "out = torch.tril(torch.ones(5, 5)) #tril is triangle-lower\n",
    "print(out)\n",
    "\n",
    "out = torch.triu(torch.ones(5, 5)) #tril is triangle-upper\n",
    "print(out)\n",
    "\n",
    "out = torch.zeros(5, 5).masked_fill(torch.tril(torch.ones(5, 5)) == 0, float('-inf'))\n",
    "print(out)\n",
    "print(torch.exp(out))\n",
    "\n",
    "#Transposing - Flips any dimension that we want\n",
    "input = torch.zeros(2, 3, 4)\n",
    "out = input.transpose(0, 2)\n",
    "print(out.shape)\n",
    "\n",
    "#Torch.Stack - Stacks tensors \n",
    "tensor1 = torch.tensor([1, 2, 3])\n",
    "tensor2 = torch.tensor([4, 5, 6])\n",
    "tensor3 = torch.tensor([7, 8, 9])\n",
    "# Stack the tensors along a new dimension\n",
    "stacked_tensor= torch.stack([tensor1, tensor2,tensor3])\n",
    "print(\"Stacked Tensor: \" + str(stacked_tensor))\n",
    "\n",
    "# nn.Linear - Takes anything that has learnable parameters and when you apply a weigh or bias it will learn those & train based on how close they are to the desired output\n",
    "#import torch.nn as nn\n",
    "sample = torch.tensor([10.,10.,10.])\n",
    "linear = nn.Linear(3, 3, bias=False)\n",
    "print(linear(sample))\n",
    "\n",
    "# Softmax Function\n",
    "#import torch.nn.functional as f\n",
    "tensor1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "softmax_output = F.softmax(tensor1, dim=0)\n",
    "print(softmax_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Vectors\n",
    "\n",
    "### What is nn.Embedding?\n",
    "\n",
    "nn.Embedding is a part of the PyTorch library and it's used to represent words or characters in a way that a machine can understand. Imagine you have a big book, and you want to teach a computer to understand the meaning of each word. nn.Embedding helps in converting these words into numbers so the computer can work with them easily.\n",
    "\n",
    "Think of it like a secret code for words. Each word gets its own special number, and the computer uses these numbers to understand and work with the words. This way, the computer can process and analyze text more efficiently. It works the same way for characters, which our model will work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 100])\n"
     ]
    }
   ],
   "source": [
    "#mport torch\n",
    "#import torch.nn as nn\n",
    "vocab_size = 1000\n",
    "embedding_dim = 100\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "#Create some input indicies\n",
    "input_indicies = torch.LongTensor([1, 5, 3, 2])\n",
    "                                  \n",
    "#Apply the embedding payer\n",
    "embedded_output = embedding(input_indicies)\n",
    "\n",
    "#The output will be a tensor of a shape (4, 100), hwere 4 is the number of inputs\n",
    "#and 100 is dimensionality of the embedding vectors\n",
    "print(embedded_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 27,  30,  33],\n",
      "        [ 61,  68,  75],\n",
      "        [ 95, 106, 117]])\n"
     ]
    }
   ],
   "source": [
    "# Dot prodcut & Matrix multiplication\n",
    "a = torch.tensor([[1,2], [3,4], [5,6]])\n",
    "b = torch.tensor([[7,8,9], [10,11,12]])\n",
    "print(a @ b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
